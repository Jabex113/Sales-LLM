# Model Architecture Configuration

model:
  name: "SalesLLM-50M"
  architecture: "GPT"
  
  # Model dimensions
  vocab_size: 32000  # Will be determined by tokenizer
  max_seq_length: 2048
  embedding_dim: 512
  num_layers: 8
  num_heads: 8
  ff_dim: 2048  # Feed-forward dimension
  dropout: 0.1
  
  # Estimated parameters: ~50M

# Training Configuration
training:
  batch_size: 8
  gradient_accumulation_steps: 4  # Effective batch size: 32
  learning_rate: 3.0e-4
  weight_decay: 0.1
  max_epochs: 20
  warmup_steps: 500
  max_steps: -1  # -1 means train for max_epochs
  
  # Optimization
  optimizer: "AdamW"
  lr_scheduler: "cosine"
  max_grad_norm: 1.0
  
  # Checkpointing
  save_every: 1000
  eval_every: 500
  
  # Hardware
  device: "cuda"  # or "mps" for Mac M1/M2, or "cpu"
  mixed_precision: true
  compile_model: false  # PyTorch 2.0 compile

# Data Configuration
data:
  train_split: 0.9
  val_split: 0.1
  
  # Data sources
  sources:
    - chat_logs
    - pdfs
    - images_ocr
  
  # Preprocessing
  min_text_length: 50
  max_text_length: 4096

# Tokenizer
tokenizer:
  type: "BPE"  # Byte-Pair Encoding
  vocab_size: 32000
  special_tokens:
    - "<|endoftext|>"
    - "<|pad|>"
    - "<|unk|>"
    - "<|user|>"
    - "<|assistant|>"
    - "<|system|>"
